## Neural Language Model

Tested on *PTB dataset* with perplexity: **116.114** using the below hyperparams.

Hyperparameters:
- Batch size: 20
- Sequence length: 20
- Number of layers: 2
- Number of hidden units: 100
